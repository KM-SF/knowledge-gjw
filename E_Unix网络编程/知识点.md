
## 1. [子进程、父进程，继承什么？没继承什么？](http://blog.chinaunix.net/uid-31415216-id-5759896.html)

1. 继承
	- 进程的资格(真实(real)/有效(effective)/已保存(saved) 用户号(UIDs)和组号(GIDs))
	- 环境(environment)
	- 堆栈
	- 内存
	- 打开文件的描述符(注意对应的文件的位置由父子进程共享， 这会引起含糊情况)
	- 执行时关闭(close-on-exec) 标志 (译者注：close-on-exec标志可通过fnctl()对文件描 述符设置，POSIX.1要求所有目录流都必须在exec函数调用时关闭。更详细说明， 参见《UNIX环境高级编程》 W. R. Stevens, 1993, 尤晋元等译(以下简称《高级编程》), 3.13节和8.9节)
	- 信号(signal)控制设定
	- nice值 (译者注：nice值由nice函数设定，该值表示进程的优先级， 数值越小，优先级越高)
	- 进程调度类别(scheduler class) (译者注：进程调度类别指进程在系统中被调度时所属的类别，不同类别有不同优先级，根据进程调度类别和nice值，进程调度程序可计算出每个进程的全局优先级(Global process prority)，优先级高的进程优先执行)
	- 进程组号
	- 对话期ID(Session ID) (译者注：译文取自《高级编程》，指：进程所属的对话期 (session)ID， 一个对话期包括一个或多个进程组， 更详细说明参见《高级编程》 9.5节)
	- 当前工作目录
	- 根目录 (译者注：根目录不一定是“/”，它可由chroot函数改变)
	- 文件方式创建屏蔽字(file mode creation mask (umask)) (译者注：译文取自《高级编程》，指：创建新文件的缺省屏蔽字)
	- 资源限制
	- 控制终端
2. 未继承
	- 进程号
	- 不同的父进程号(译者注： 即子进程的父进程号与父进程的父进程号不同， 父进程号可由getppid函数得到)
	- 自己的文件描述符和目录流的拷贝(译者注： 目录流由opendir函数创建，因其为顺序读取，顾称“目录流”)
	- 子进程不继承父进程的进程，正文(text)， 数据和其它锁定内存(memory locks) (译者注：锁定内存指被锁定的虚拟内存页，锁定后， 不允许内核将其在必要时换出(page out)， 详细说明参见《The GNU C Library Reference Manual》 2.2版， 1999, 3.4.2节)
	- 在tms结构中的系统时间(译者注：tms结构可由times函数获得， 它保存四个数据用于记录进程使用中央处理器 (CPU：Central Processing Unit)的时间，包括：用户时间，系统时间， 用户各子进程合计时间，系统各子进程合计时间)
	- 资源使用(resource utilizations)设定为0
	- 阻塞信号集初始化为空集(译者注：原文此处不明确， 译文根据fork函数手册页稍做修改)
	- 不继承由timer_create函数创建的计时器
	- 不继承异步输入和输出



## 2. 同一个进程下的所有线程，共享什么？私有什么？

1. 共享
   1. 虚拟地址空间
   2. 进程的公有数据\全局数据
   3. ... ...
2. 私有什么
   1. 寄存器
   2. 线程的堆栈
   3. errno
   4. 线程的信号屏蔽码
   5. 线程优先级



---

### /proc文件系统中

- 提供系统中【进程】的信息

  > /proc/\<pid\>

- 目录（用作组织信息的方式）和虚拟文件
- cpuinfo：标识了处理器的类型和速度
- meminfo ：内存信息
- pci：显示在PCI总线上找到的设备
- modules：标识了当前加载到内核中的模块
- devices：块设备、字符设备

## 进程

不可中断进程(Disk-Sleep) / 僵尸进程(Zombie) https://www.cnblogs.com/agilestyle/p/11520274.html

1. 不可中断状态: 表示进程正在跟硬件交互，为了保护进程数据和硬件的一致性，系统不允许其他进程或中断打断这个进程。进程长时间处于不可中断状态，通常表示系统有 I/O 性能问题。
2. 僵尸进程: 表示进程已经退出，但它的父进程还没有回收子进程占用的资源。短暂的僵尸状态通常不必理会，但进程长时间处于僵尸状态，就应该注意了，可能有应用程序没有正常处理子进程的退出。




## [Linux惊群](https://blog.csdn.net/sinat_35297665/article/details/80569656)

1. 惊群效应消耗了什么
    1. 系统对用户进程/线程频繁地做无效的调度，上下文切换系统性能大打折扣。
    2. 为了确保只有一个线程得到资源，用户必须对资源操作进行加锁保护，进一步加大了系统开销。
2. 惊群场景
   1. accept惊群
      1. 构造场景
         - 主进程创建了socket、bind、listen之后，fork()出来多个进程，每个子进程都开始循环处理（accept）这个listen_fd。每个进程都阻塞在accept上，当一个新的连接到来时候，所有的进程都会被唤醒，但是其中只有一个进程会接受成功，其余皆失败，重新休眠。
      2. Linux2.6已经解决了该问题
         - 解决方案：当内核接收到一个客户连接后，只会唤醒等待队列上的第一个进程（线程）
    2. epoll惊群
       1. 构造场景
          - 主进程创建socket，bind，listen后，将该socket加入到epoll中，然后fork出多个子进程，每个进程都阻塞在epoll_wait上，如果有事件到来，则判断该事件是否是该socket上的事件如果是，说明有新的连接到来了，则进行接受操作。 ==> epoll_wait依然存在惊群
    2. 为什么内核处理了accept惊群，却不处理epoll_wait惊群呢？答：这与二者的使用场景有关
       1. accept是接收连接请求，只需要一个接收成功就ok
       2. epoll_wait除了可以接收连接请求，还可以接收IO读写事件的到来。其他IO事件能否只能由一个进程处理，是不一定的，内核不能保证这一点，这是一个由用户决定的事情（例如，一个文件可以由多个进程来读写）==> 所以，对epoll的惊群，内核则不予处理
    3. Nginx怎么解决epoll_wait惊群

---

## cpu三级缓存

- CPU缓存是什么？
  - 为了解决CPU运行处理速度 >> 内存读写速度的矛盾
  - 我们来简单地打个比方：如果CPU在L1一级缓存中找到所需要的资料要用的时间为3个周期左右，那么在L2二级缓存找到资料的时间就要10个周期左右，L3三级缓存所需时间为50个周期左右；如果要到内存上去找呢，那就慢多了，可能需要几百个周期的时间。

- Cache存在的意义
  - 作为CPU与内存之间高速数据缓冲区
  - L1最靠近CPU核心，运行速度最快
- 原理
  - 局部性原理（最近访问的数据，将大概率被再次访问）

1. 结构图
```
                           CPU
           CPU核心                            CPU核心

1级指令cache      1级数据cache       1级指令cache      1级数据cache

          2级cache                          2级cache
                         3级cache

                           内存
``` 
2. 三种不一致性问题
   1. 一个cpu核心中的“1级指令cache、数据cache”之间的不一致性
      - 对于程序代码运行而言，指令都是经过一级指令cache，而指令中涉及到的数据都是经过数据cache ==> 所以，对修改运行中代码指令数据（如修改内存地址A这个位置的代码）而言，这个时候是通过存储的方式去写的地址A，所以新的指令会进入数据cache ==> 但是，接下来去执行A处的指令的时候，指令cache里面可能命中的是修改之前的指令。所以，此时软件需要(1)把数据cache中的数据写入到内存中，(2)然后让指令cache无效，(3)重新加载内存中的数据
   2. 多个cpu核心，各自2级cache的不一致性
      - 因为两个2级cache共享一个3级cahce，当第一个CPU的2级cache已经读取了A地址的变量，那么，当第二个CPU的2级cache去读取了A地址的变量时，应该怎么办呢？第二个CPU核心是不是需要从内存里面经过第3、2、1级cache再读一遍，这个显然是没有必要的
       - 在硬件上cache相关的控制单元，可以把第一个cpu核心的A地址处cache内容直接复制到第二个cpu的第2、1级cache，这样两个cpu核心都得到了A地址的数据
       - 不过，如果这时第一个cpu核心改写了A地址处的数据，而第二个cou核心的2级cache里面还是原来的值，数据显然就不一致了（为了解决这些问题，硬件工程师开发了多核心cache数据同步协议，如MESI、MOESI）
   3. cpu的3级cache与设备内存（如，DMA、网卡、显存）之间的一致性


---

## 页面置换算法

1. 最佳置换算法：（理想）将以后不使用的页面，置换出去
2. FIFO 先进先出页面置换算法：将最先进入的页面换出
3. Clock 时钟：页面形成环形链表，每次将指针指向的页面换出
4. LRU 最近最久未使用（Least Recently Used）
5. LFU 最近最少使用（Least Frequently Used）

---

## linux内存管理与分配

内部碎片: 是指被内核分配出去但是不能被利用的内存

外部碎片: 是指由于频繁地申请和释放页框而导致的某些小的连续页框，无法分配给需要大的连续页框的进程而导致的内存碎片

- 伙伴算法

- slab分配


---

## buffer / cache

1. buffers
   1. 内核缓冲区用到的内存（磁盘块设备）
   2. 对原始磁盘块设备的临时存储，即：用来缓存磁盘的数据，通常不会特别大(20MB左右)
2. cache：内核页缓存和slab用到的内存
   1. 读写文件的页缓存（文件系统）

对比实验
1. dd if=/dev/urandom of=/tmp/file bs=1M count=1024   buffer没变化，cache增长明显 
   1. of=文件系统的文件
2. dd if=/dev/vda1 of=/dev/null bs=1M count=1024      buffer增长明显，cache几乎不变
   1. of=块设备文件

---

## 函数调用栈

先了解3个寄存器: bp、sp、ip，作用分别是指向栈起始地址、指向当前栈顶、指向下一条将要运行的指令

1. 每个函数的栈在bp和sp指针之间
2. 存在调用关系的两个函数的栈内存地址是相邻的
3. bp指针指的位置存储的是上级函数的bp地址（目的是在执行被调函数前，先保存上级函数的bp，当被调函数执行后，可以用来恢复上级函数的bp）

每个函数栈帧
1. bp: 栈帧基址（指向上级函数的bp）
2. 局部变量
3. 返回地址
4. 参数

说明: call=调用函数，called=被调函数

1. 初始化: call函数的函数栈帧已经形成
2. call指令 
   1. called的返回地址保存到栈帧
   2. 跳转到called指令开始地址，此时ip的值为“called的第一条指令”
3. 被调函数执行过程
   1. b1指令:先把sp指针向下移动24字节，为自己分配足够大的栈帧
   2. b2指令:将call's bp保存到栈帧中
   3. b3指令:把called's bp存入bp寄存器
   4. 接下来就是执行函数b指令:called的参数入栈... ...
4. 在ret指令之前，编译器还会插入2条指令
   1. 将bp的值，恢复为call's bp (该值之前存储在栈帧中)
   2. 通过移动sp指针，来释放自己的栈帧空间:分配时向下移动多少，释放时就向上移动多少
   - 此时，bp和sp寄存器保存到值分别为“指向call’s bp”和“called的返回地址”
5. ret指令
   1. 从栈帧中，弹出“called的返回地址”，此时，sp寄存器保存的值，恢复到初始化时的状态(call函数栈帧刚形成)
   2. 设置下一条指令ip的值为“跳转到函数的返回地址”
6. 此时，called执行完毕，现在从called返回地址这里，继续向下执行了


---


## [零拷贝](https://time.geekbang.org/column/article/232676)


1. 引出零拷贝的原因
   - 基于用户缓冲区传输文件时，过多的内存拷贝与上下文切换次数，会降低性能
   - 零拷贝技术是在内核中完成内存拷贝，天然降低了内存拷贝次数
   - 它通过一次系统调用欧冠合并了磁盘读取与网卡发送两个操作，降低了上下文切换次数
   - 由于拷贝在内核中完成，它可以最大化使用socket缓冲区的可用空间，从而提高了一次系统调用中处理的数据量，进一步降低了上下文切换次数 
2. 零拷贝不适用的场景
   1. 不允许进程对文件内容作一些加工再发送，如，数据压缩后发送
   2. page-cache会引发副作用，也不能使用零拷贝，此时可以使用异步IO替换
      1. 通常对文件设定阈值
      2. 超过阈值的大文件，采用异步IO和直接IO；小文件，采用零拷贝
1. 零拷贝函数
   - ssize_t senfile(int out_fd,int in_fd,off_t* offset,size_t count);
      1. in_fd ==> out_fd
      2. in_fd必须是一个支持类似mmap函数的文件描述符，即，它必须指向真实的磁盘文件，不能是socket和管道
      3. out_fd必须是一个socket
2. 传统的一次读写：4次拷贝，4次上下文切换
    - 磁盘 ==> page-cache ==> 用户态用户缓冲区 ==> socket缓冲区 ==> 网卡
    - 上下文切换：代码执行read/write，一定会发生2次上下文切换
        - 从用户态切换到内核态，当内核执行完任务后，再切换回用户态交由进程代码执行
        - 因此，如果想减少上下文切换次数，一定要减少系统调用次数 
   - 分析：
      1. 与物理设备相关的2次拷贝是必不可少的
           - 磁盘 ==> page-cache
           - socket ==> 网卡
      2. 但另外 2 次与用户缓冲区相关的拷贝动作都不是必需的，因为在把磁盘文件发到网络的场景中，用户缓冲区没有必须存在的理由
          1. 如果内核在读取文件后，直接把 PageCache 中的内容拷贝到 Socket 缓冲区，待到网卡发送完毕后，再通知进程，这样就只有 2 次上下文切换，和 3 次内存拷贝
          2. 如果网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术，还可以再去除 Socket 缓冲区的拷贝，这样一共只有 2 次内存拷贝

## 异步IO + 直接IO
1. 高并发场景处理大文件，应该使用异步IO和直接ID来替换零拷贝技术
2. 异步IO
   1. 它把读操作分为两部分，前半部分向内核发起读请求，但不等待数据就位就立刻返回，此时进程可以并发地处理其他任务。
   2. 当内核将磁盘中的数据拷贝到进程缓冲区后，进程将接收到内核的通知，再去处理数据
3. 直接IO
   1. 使用场景（并不多，主要有两种）
      1. 应用程序已经实现了磁盘文件的缓存，不需要page-cache再次缓存，引发额外的性能消耗，如：
         1. sql等数据库直接使用IO
         2. 高并发下传输大文件（因为大文件无法命中page-cache缓存，又会带来额外的内存拷贝，同时还挤占了小文件使用page-cache时需要的内存），因此应该使用直接IO
   2. 直接IO缺点
      1. 非直接IO情况下
         1. 除了缓存外，内核（IO调度算法）会视图缓存尽量多的连续IO在page-cache中，最后合并成一个更大的IO再发给磁盘，这样可以减少缓存的寻址操作
         2. 内核会预读后续的IO放在page-cahce中，减少磁盘操作
      2. 直接IO
         - 直接绕过了page-cache，无法享受性能提升


**EAGAIN错误码**: 提示本次失败，提示重试（下一次可能会成功）。 ==> 非阻塞，read数据时，没数据可读（此时，<u>内核缓冲区中没有准备好的数据到来</u>），不会阻塞，直接会返回EAGAIN错误，告诉你此时没数据，之后可以重试read

---

## [案例] [单机如何实现管理百万主机的心跳服务](https://time.geekbang.org/column/article/240656) 


1. 背景：主机理想情况下，每秒上报一次心跳包。知道了主机最近一次上报的时间，与now()对比，就能知道该主机是否为宕机。
2. 解法：单链表（按照时间有序）
   1. item: 保存了每个主机最近上报的时间, (机器ID, 最近一次上报时间)
   2. 算法
        ```c
            // 查询那些主机宕机了，如果宕机了，执行告警
            for (auto item : list) {
                // 没超时
                if (now() - item.Time < 5s) {
                    return // 之后的节点肯定都是在线的(因为是按照时间有序的)
                } else { // 超时
                    // 上报机器已经超时了
                }
            }
        ```


---

## [CAS、ABA](https://blog.csdn.net/justry_deng/article/details/83449038)

1. CAS(\*V, old, new): 若`V指针指向的值==old`时，\*V=new
2. ABA问题
   1. 现象
      1. 将V的值从old改成new是需要花时间的
      2. 假设在这个时间戳口内，V指向的值可能被修改成xx，又立刻被回来（改成old）
      3. 改回来后，CAS(\*V, old, new)依然能执行成功 ==> 但是此时的V已经变化了，我们却没感知到
   2. 解决方案：引入版本号，①除了对比*V==old之外，②还要对比版本号
      - 当V发生变化时，版本号+1
      - 执行CAS时，对比：版本号是否一致 && *V==old


---

## 优化TCP三次握手
1. client
   1. 重试策略：1/2/3/6/16/32/64重试6次 ==>根据业务逻辑，改造重试策略
2. server
   1. syn半链接队列
      - 溢出后，就无法建立新的连接 ==> 开启synccookies功能，在不使用syn队列的情况下成功建立连接
        - syncookies 是这么做的：服务器根据当前状态计算出一个值，放在己方发出的 SYN+ACK 报文中发出，当客户端返回 ACK 报文时，取出该值验证，如果合法，就认为连接建立成功
          - Linux 下怎样开启 syncookies 功能呢？修改 tcp_syncookies 参数即可，其中值为 0 时表示关闭该功能，2 表示无条件开启功能，而 1 则表示仅当 SYN 半连接队列放不下时，再启用它。由于 syncookie 仅用于应对 SYN 泛洪攻击（攻击者恶意构造大量的 SYN 报文发送给服务器，造成 SYN 半连接队列溢出，导致正常客户端的连接无法建立），这种方式建立的连接，许多 TCP 特性都无法使用。所以，应当把 tcp_syncookies 设置为 1，仅在队列满时再启用
    2. 当client发给server的最后一个ack丢失时，server会重发syn+ack，并一直重试，等待的时间很长，解决方案
       1. 修改重试时间策略（参考client1.1）
       2. server发送rst给客户端，告诉连接失败

## 优化TCP四次挥手
1. 主动关闭方
   - 关闭方式有很多种，比如进程异常退出，针对它打开的链接，内核就会发送rst报文来关闭（rst不走四次挥手强制关闭，但是当报文延迟或者重复传输时，这种方式会导致数据错乱，所以这是不得已而为之的关闭方案）

---

1. RST分节、缓冲区
   1. A调用close关闭全双工连接过程：要等待缓冲区的数据全部收发完毕，才断开
   2. RST分节、SO_LINGER标记
      - A向B发送了RST分节后，如果缓冲区中有数据，那么缓冲区中的数据将会被直接丢弃，然后A端断开连接。此时B收到RST分节后，将会直接断开连接
2. TCP序列号回绕问题
   - 一个tcp流的初始序列号（ISN）并不是从0开始的，而是采用一定的随机算法产生的，因此ISN可能很大（比如(2^32-10)），因此同一个tcp流的seq号可能会回绕到0。
   - 而我们tcp对于丢包和乱序等问题的判断都是依赖于序列号大小比较的。此时就出现了所谓的tcp序列号回绕问题。
   - 内核解决办法
      ```c
      static inline int before(__u32 seq1, __u32 seq2)  // __u32 无符号整型
      {
         return (__s32)(seq1-seq2) < 0;  // __s32有符号整形
      }
      #define after(seq2, seq1) before(seq1, seq2)
      ```
1. bind：绑定的是IP/port
   > 服务端：绑定共用的IP/port，使得其他cli能够连接
   >
   > 客户端：不需要bind，OS会分配一个临时port给它，IP使用的是自己的
2. UDP connect
   1. 不会经过3次握手，内核只是检查是否存在立即可知的错误(例如一个显然不可达的目的地)，记录对端的IP地址和端口号（取自传递给connect的套接口地址结构），然后立即返回到调用进程
   2. 减少每次断开、连接的消耗
   3. 多次调用：断开旧连接，创建新连接
   4. 接收到异步错误（不能禁用ICMP）：内核记录住connect中目的IP/PORT，以后就可以read/write同时内核会告诉所连接的套接字的异步错误 ==>[补充] 如果是“非connect”的话，内核也会收到这个ICMP。因为发送端可以向多个目的地址发送数据包，当没有调用connect绑定IP/PORT，因此，内核没有记录IP/PORT的信息，无法知道这个ICMP应该返回给那个套接字，可以理解为，内核无法处理这种情况
3. 端口只有65536个，那么连接只能建立这么多吗?
   1. 错误言论：因为TCP端口号是16位无符号整数, 最大65535, 所以一台服务器最多支持65536个TCP socket连接
   2. 答案：[一个服务器最多有多少个连接](https://www.cnblogs.com/cangqinglang/p/14131541.html)
      > 应该受TCP连接里四元组的空间大小限制，算起来是200多万亿个，即：TCP连接四元组是**源IP地址、源端口、目的IP地址、目的端口**。
      > 
      > 任意一个元素发生了改变，那么就代表的是一条完全不同的连接了。
      >
      > 拿Nginx举例，它的端口是固定使用80，另外，我的IP也是固定的，这样目的IP地址、目的端口都是固定的。<u>剩下源IP地址、源端口是可变的</u>。所以理论上我的Nginx上最多可以建立<u>2的32次方(ip数)×2的16次方(port数)</u>个连接。这是200多万亿的一个大数字
   3. 单机最大TCP连接数受到哪方面的限制
      1. 内存
      2. 文件描述符数量限制
         - 系统级
         - 用户级
         - 进程级：1024，是单个进程哦，而不是整个系统
4. [一个TCP连接占用多少内存](https://zhuanlan.zhihu.com/p/25241630)
   1. 答案：大约是4KB左右，TCP控制块
   2. 说明：错误的人经常会认为，至少应该是10KB=“2KBTCP控制块+4KB读缓冲区+4KB写缓冲区”
   3. 分析：
      1. 连接在建立时并不会真的去分配内存，而是在使用的时候才会被分配内存
      2. 每个 TCP socket 占用的内存最少是 256 + 192 + 640 + 1792 + 64 = 2944 字节
         ```c
         | struct           | size | slab cache name    |
         | ---------------- | ---- | ------------------ |
         | file             |  256 | "filp"             | 对应每个打开的文件
         | dentry           |  192 | "dentry"           | 文件所在的目录
         | socket_alloc     |  640 | "sock_inode_cache" | 
         | tcp_sock         | 1792 | "TCP"              | 
         | socket_wq        |   64 | "kmalloc-64"       | 
         | ---------------- | ---- | ------------------ |
         | inet_bind_bucket |   64 | "tcp_bind_bucket"  |
         | epitem           |  128 | "eventpoll_epi"    |
         | tcp_request_sock |  256 | "request_sock_TCP" |
         ```
5. TCP: 紧急指针、带外数据
   - 带外数据
     > 又称为紧急数据，只能是<u>一个字节</u>的数据（
     > 
     > TCP支持带外数据，但是只有一个OOB字节，TCP的带外数据是通过紧急模式URG实现的.）
   - 紧急指针
     > 紧急指针并不存放带外数据，而是存放指针，该指针指向带外数据位置的下一个位置
6. TCP的4种计时器
   1. 超时重传计时器
      - 发送数据包，启动计时器
   2. 持续计时器
      - 发送端接收到零窗口，停止发送数据，开启定时器，到时时，发送探测窗口大小的数据包
   3. 保活计时器
      - 默认超时通常设置为2小时（当服务器超过了2h还没有收到客户的任何信息时，服务器就向客户发送过一个探测报文段。若连续发送了10个探测报文段（每个75s一个）还没有响应，就认为客户出了故障，并终止连接
   4. TIME_WAIT计时器
      - 接收到第二个FIN后，启动TIME_WAIT计时器
  
---

## SO_REUSEADDR \ [SO_REUSEPORT](https://www.cnblogs.com/Anker/p/7076537.html)


1. SO_REUSEADDR 地址复用
   1. 场景：服务端，调用bind之前，先设置SO_REUSEADDR
      - 一个套接字由相关五元组构成，协议、本地地址、本地端口、远程地址、远程端口。SO_REUSEADDR 仅仅表示可以重用本地本地地址、本地端口，整个相关五元组还是唯一确定的
   2. 作用：通知内核，如果端口忙，但TCP状态位于 TIME_WAIT ，可以重用端口。如果端口忙，而TCP状态位于其他状态，重用端口时依旧得到一个错误信息，指明"地址已经使用中"。如果你的服务程序停止后想立即重启，而新套接字依旧使用同一端口，此时SO_REUSEADDR 选项非常有用。必须意识到，此时任何非期望数据到达，都可能导致服务程序反应混乱，不过这只是一种可能，事实上很不可能
2. SO_REUSEPORT Linux最新特性
   1. 场景：支持多个进程/线程绑定到同一个端口，提高服务器的性能。解决的问题：
      1. 允许多个socket bind()/listen()同一个TCP/UDP端口
      2. 内核层面实现负载均衡
      3. 安全层面，监听同一个端口的套接字只能位于同一个用户下面
   2. 有了SO_RESUEPORT后，每个进程可以自己创建socket、bind、listen、accept相同的地址和端口，各自是独立平等的。让多进程监听同一个端口，各个进程中accept socket fd不一样，有新连接建立时，“内核只会唤醒一个进程”来accept，并且保证唤醒的均衡性
